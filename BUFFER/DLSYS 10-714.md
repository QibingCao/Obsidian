---

类型: 笔记
创建日期: 2024-07-29
修改日期: 2024-07-29
---
# linear hopothesis
## supervised learning
对于类似手写数字分类的任务，对于机器来说无法像人类一样很直观的看到图像，人类无法很严谨地给出数字图像的定义。
但是我们能很容易的用图片和已经做好的标签去训练一个系统，让他的识别正确率尽可能的高
![[Pasted image 20240729151752.png]]

我们想让模型输出分类的结果，相比于直接对为一错为零这种无法优化的loss（小范围的输出变化不会影响结果，模型不找不到减少loss的方向），让模型输出为某个类的概率，以便优化。
### softmax
又因为输出的结果是实数，为了让输出满足概率的性质（0到1之间，概率总和为1）于是给每个维度的输出都套一层e指数，并除以总和。

![[Pasted image 20240729160036.png]]

![[Pasted image 20240729193241.png]]针对于所有的监督学习来说，我们要最小化所有的loss，此处为softmax loss。
![[Pasted image 20240729194512.png]]
最小化的过程相当于求参数矩阵θ的梯度，此时将loss函数看作是θ的函数f(θ)。
此时根据已有的输出结果，求出θ的梯度，梯度的每一项都是偏导，**即固定其他维度的参数不变，求此处位置的导数**。
倒三角下面的θ指的是对θ求导。
梯度代表了在当前位置函数向值最增大的方向，想要最小化那就要向梯度的相反方向。
![[Pasted image 20240729194450.png]]
理论上做法应该是，对于训练集上的所有输入样本，分别求出每个样本对应的梯度，然后**累加**得到全局的梯度，再用全局梯度更新θ。
但是在实际使用中并不是这样做的。完整的计算全部样本的梯度对于大量数据来说是不现实的。实际上是用一小部分的数据求出梯度然后更新θ，这样就是大量的θ数据更新而不是大量梯度计算。

小批量替代整体的前提是无偏估计和独立同分布。本来数据集就是对某一个物理规律的数据采样，他也不是整个物理规律应用的全部数据来源。所以数据集就是符合无偏估计和独立同分布的。
可以把小批量理解为整个数据集的带噪声的采样。
![[Pasted image 20240729202208.png]]
### 随机梯度下降（Stochastic Gradient Descent, SGD）
随机梯度下降通过在每次迭代中只使用一个或一小部分样本来近似地计算梯度，从而进行参数更新。
#### 步骤：
1. **初始化**：随机初始化模型参数。
2. **随机选择样本**：从训练数据集中随机选择一个样本 xix_ixi​ 及其对应的标签 yiy_iyi​。
3. **计算梯度**：计算选定样本上的损失函数的梯度。
4. **更新参数**：根据计算出的梯度更新参数。 θ:=θ−η∇θJ(θ;xi,yi)\theta := \theta - \eta \nabla_\theta J(\theta; x_i, y_i)θ:=θ−η∇θ​J(θ;xi​,yi​)
#### 特点：
- **单样本更新**：每次迭代只使用一个样本，更新频繁。
- **计算效率高**：每次更新计算成本低，适合大型数据集。
- **噪声大**：由于使用单样本进行更新，每次更新方向可能会偏离全局最优方向，导致收敛过程中的抖动。
- **更快收敛**：虽然每次更新带有噪声，但在大多数情况下，SGD能比GD更快到达最优点，尤其是在非凸优化问题中。

### 梯度计算
loss对输出结果的导数
![[Pasted image 20240729204458.png]]
导数就是输出结果减去真实值
![[Pasted image 20240729204447.png]]
#### 对θ求导
真实在做的：假设所有参数都是标量，采用经典的链式法则求出结果并相加。这种方法在实际应用中更常用，但需要数值验证来确保其正确性。
![[Pasted image 20240729205449.png]]
直接当做标量用链式法则，前半段就是前面过的对h偏导，后面就是x本身。注意维度不同，我们要的梯度维度应该等同于θ的维度n，k，所以直接给上述结果换换位置相乘，难绷
![[Pasted image 20240729211359.png]]

发现梯度是真滴好求，结果都是算好的，直接乘起来就完事了
![[Pasted image 20240729212348.png]]
#### 插曲
在看softmax对h求导时，发现长得巨像sigmoid，接着又发现sigmoid和softmax本来就是一个东西，softmax当类别只有两个时，就是sigmoid

# neural network
对于无法使用线性假设来分类的问题
一个思路是可以说明，任意维度的线性变换基本上是等价的。在原本的线性变换基础上增加新的线性矩阵对结果不起作用。
因此我们可以在线性变换之后，在添加非线性的变换。