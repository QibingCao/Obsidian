---

类型: 笔记
创建日期: 2024-04-02
修改日期: 2024-04-02
---
[原文链接segmenfault](https://segmentfault.com/a/1190000039676522)
# 1. epoll下共享监听端口的行为
在使用epoll的情况下，也存在多个进程一起监听端口的情况，最经典的例如nginx，多个worker会一起监听同一个端口，而在它1.11版本之前，使用的是上述第一节讲的方式，master进程创建一个监听端口之后，通过fork的方式，让woker进程继承这个端口，然后放到epoll里面去进行监听，现在我们重点来看一下在这个场景下（epoll+accept）内核的行为是怎样的  
与直接accept不同，epoll需要先调用epoll_create在内核中创建一个epoll文件，如下图
![[1460000039676527.webp]]
epoll会创建一个匿名的inode节点，这个节点指向的是一个epoll主结构，这个结构中有两个核心字段，一个是红黑树，用户态需要监听的文件都会挂在这个红黑树下面，实现lgn的查找、插入、更新的复杂度；另外一个是rdlist，即文件事件的就绪队列，指向的是一个链表，事件产生的时候，epoll会把对应的epitem（即红黑树上的节点）插入到这个链表中，当向用户态返回的时候，只需要遍历这个就绪链表即可，而不需要像select那样遍历所有文件，不过本文的重点是分析epoll的阻塞及唤醒过程，epoll本身的主结构简单带过，实际上这个结构是比较复杂的，可以看我之前写过的 [文章](https://link.segmentfault.com/?enc=6F85%2FI7IqGqXXMo8gydLsw%3D%3D.vC7mdg8SkogCoCnJuMRBVLSEjHLOn0EgjUs7qrRPVyla8H5%2FHQUS%2Fu7COcIbRuAI)

接下来看如何把要监听的socket fd挂在epoll上，这个过程调用的是epoll_ctl，将fd向内核传递，内核实际上会做两个事情

1. 将fd挂在红黑树中
2. 调用文件设备驱动的poll回调指针（这是重点）

epoll/select等这些模型要实现多路复用，实际上主要就是依赖于：将进程挂在对应的fd的等待队列上，这样当这个fd的事情产生的时候，设备驱动就会将这个队列上的进程唤醒，如果进程不依赖epoll，毫无疑问他无法将自己同时挂在多个fd的队列上，epoll帮他干了这个事情，而干这个事情的一个核心步骤，是调用对应fd驱动设备提供的poll方法  
linux中，对设备模型进行了一个规范的标准化，比如设备分为字符设备、块设备、网络设备等，对于开发者而言，要给一个设备实现一个驱动程序就必须按照linux提供的规范来实现，其中对于跟用户层交互这块，内核要求开发者实现一个叫file_operations的结构，这个结构定义了一系列操作的回调指针，比如read、write等用户熟知的操作，当用户调用read、write等方法时，最终内核会回调到这个设备的file_operations.read、file_operations.write方法，这个方法的具体逻辑需由驱动开发者实现，比如本文的accept调用，实际上最终是调用了socket下面的file_operations.accept方法  
综上所述，如果一个设备要支持epoll/select的调用，他必须实现file_operations.poll方法，epoll在处理用户层传入的fd时，实际上最终是调用了这个方法，而这个方法linux同样做了一系列规范，他要求开发者实现以下逻辑：

1. 要求poll方法返回用户感兴趣的事情的标志，比如当前fd是否可读、是否可写等
2. 如果poll传入一个poll专用的等待队列结构体，那他将会调用这个结构体，这个结构体中会有一个叫poll_table的东西，里面有一个回调函数，poll方法最终会调用这个回调，这个回调是由epoll来设定的，epoll在这个方法中实现的逻辑是：将当前进程挂在这个fd的等待队列上面

简单来说，如果是进程自己调用accept，则协议栈驱动会亲自把这个进程挂在等待队列上，如果是epoll来调用，则会回调poll方法，最终epoll亲自将进程挂在这个等待队列上面，记住这个结论，这是引发accept惊群效应的最根本原因  
我们来看一下epoll是如何跟file_opreations->poll方法进行交互的，我画了一个简单的时序图
![[1460000039676526.webp]]
如上图，当用户调用epoll_ctl的添加事件的时候，在第6步中，epoll会把当前进程挂在fd的等待队列下，但是默认情况下这种挂载不会设置互斥标志，意思着当设备有事情产生进行等待队列唤醒的时候，如果当前队列有多个进程在等待，则会全部唤醒  
可想而知，在下面的epoll_wait调用中，如果多个进程将同一个fd添加到epoll中进行监听，当事件到达的时候，这些进程将被一起唤醒  
但是唤醒并不一定会向用户态返回，因为唤醒之后epoll还要遍历一次就绪列表，确认有至少一个事件发生才会向用户态返回  
到此，我们可以想象出epoll是如何造成accept惊群的：

1. 当多个进程共享同一个监听端口并且都使用epoll进行多路复用的监听时，epoll将这些进程都挂在同一个等待队列下
2. 当事件产生时，socket的设备驱动都会尝试将等待队列的进行唤醒，但是由于挂载队列的时候使用的是epoll的挂载方式，没有设置互斥标志（取代了accept自己挂载队列的方式，如第一节所述），所以这个队列下的所有进程将全部被唤醒
3. 唤醒之后此时这些进程还处于内核态，他们都会立刻检查事件就绪列表，确认是否有事件发生，对accept而言，accept->poll方法将会检查在当前的socket的tcp全连接列表中是否有可用连接，如果是则返回可用事件标志
4. 当所有进程都被唤醒，但是还没有进行去真正做accept动作的时候，所有进行的事件检查都认为accept事件可用，所以这些进行都向用户态返回
5. 用户态检查到有accept事件可用，这时他们将会真正调用accept函数进行连接的获取
6. 此时只会有一个进行能真正获取连接，其他进行都会返回EAGAIN错误，使用strace -p PID命令可以跟踪到这种错误
7. 并不是所有进行都会返回用户态，关键点在于这些被唤醒的进行在检查事件的过程中，如果已经有进程成功accept到连接了，这时别的事情将不会检查到这个事情，从而他们会继续休眠，不会返回用户态
8. 虽然不一定会返回用户态，但也造成了内核上下文切换的发生，其实也是惊群效应的表现
# 内核解决了惊群效应了吗
根本原因在于epoll的默认行为是对于多进程监听同一文件不会设置互斥，进而将所有进程唤醒，后续的内核版本主要提供了两种解决方案

1. 既然默认不会设置互斥，那就加一个互斥功能好了:-)，linux4.5内核之后给epoll添加了一个EPOLLEXCLUSIVE的标志位，如果设置了这个标志位，那epoll将进程挂到等待队列时将会设置一下互斥标志位，这时实现跟内核原生accept一样的特性，只会唤醒队列中的一个进程
2. 第二种方法：linux 3.9内核之后给socket提供SO_REUSEPORT标志，这种方式解决得更彻底，他允许不同进程的socket绑定到同一个端口，取代以往需要子进程共享socket监听的方式，这时候，每个进程的监听socket将指向open_file_tables下的不同节点，也就是说不同进程是在自己的设备等待队列下被挂起的，不存在共享fd的问题，也就不存在被同时唤醒的可能时，而内核则在驱动中将设置了SO_REUSEPORT并且绑定同一端口的这些socket分到同一个group中，当有tcp连接事件到达的时候，内核将会对源IP+源端口取hash然后指定这个group中其中一个进程来接受连接，相当于在内核级别中实现了一个负载均衡

基于以上两种方法，其实epoll生态在目前来说不存在所谓的惊群效应了，除非：你溢用epoll，比如多进程之间共享了同一个epfd（父进程创建epoll由多个子进程来调用），那就不能怪epoll了，因为这时候多个进程都被挂到这个epoll下，这种情况下，已经不是仅仅是惊群效应的问题了，比如说，A进程在epoll挂了socket1的连接事件，B进程调用了epoll_wait，由于属于同一个epfd，当socket1产生事件的时候，进程B也会被唤醒，而更严重的事情在于，在B的空间下并不存在socket1这个fd，从而把问题搞得很复杂。总结：千万不要在多线程/多进程之间共享epfd